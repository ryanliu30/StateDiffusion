# Use clipped double-Q learning (from TD3)
num_critic_networks: 2
target_critic_backup_type: min

base_config: sac

batch_size: 256
actor_learning_rate: 0.0003
critic_learning_rate: 0.0003

discount: 0.99
use_soft_target_update: true
soft_target_update_rate: 0.005

actor_gradient_type: reparametrize
num_critic_updates: 1

use_entropy_bonus: true
temperature: 0.05

num_agent_train_steps_per_iter: 50000
synthesis_batch_size: 1500
mbpo_rollout_length: 5
replay_buffer_capacity: 1000000
synthetic_to_real_ratio: 1
synthetic_amplification: 100